{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3 - cs2951-R - Motion Classification - Main\n",
    "Shaun Wallace Files: features.py, classifier.py (Python 3.5.1 & Anaconda 4, Mac OS X 10.9.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn import tree, svm, neighbors, cross_validation, metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datasets & Initial Features\n",
    "The initial raw data was gathered using AndroSensor on Android.  For each activity my phone was placed in my pocket with the screen facing inwards towards my leg.  I gathered 7 to 15 minutes of data per activity.  The beggining and end of the data was truncated to create 5 minutes of data per acitivity file.  To assess the possible features each column was compared to elapsed time.  I felt to assess the data it was easiest to view the in this context.  After reviewing these relationships I felt most data was relevant to utilize to test features.  In my processing of data I utilized every combination of XYZ coordinates for each sensor.  For each column min, max, standard deviation, and mean was computed for each 10 seconds worth of data.  \n",
    "\n",
    "A very enlighting view was looking at the testing data.  Which consited of 1 minute snippets for all activities.  This helps paint the picture about how different each activity was. Because of this I did not initially scale the data between -1 and 1.  This is because the range of the data and average value can vary greatly between each activity type: \n",
    "\n",
    "![title](img/accXtime.png)\n",
    "\n",
    "##### Features\n",
    "\n",
    "Originally I went through and computer min,max,stdev,mean, and used the resultant acceleration formula on XYZ axis for all 18 sensors.  I realized later on this was classic case of overfitting even though. My original accuracy scores were LogReg .97, Tree .98, Nearest Neighbors .935, and SVM .98.  While these were good numbers they were more a product of overfitting.  I could also correctly predict the activity in all instances except when walking and running got confused.  In order to narrow down my number features I went back to view plots of values vs time.\n",
    "\n",
    "I went through the graphs and selected several features I thought could be potentially useful:\n",
    "1. Acceleration X (min, max, stdev, mean)\n",
    "2. Average Resultant Acceleration - (Acceleration XYZ) (np.sqrt(X+Y+Z))/100.0 <-X = sum(X) * sum(X)\n",
    "3. Gravity Y (mean)\n",
    "4. Linear Acceleration Z (min, max)\n",
    "5. Average Resultant Linear Acceleration - (Linear Acceleration XYZ) (np.sqrt(X+Y+Z))/100.0 <-X = sum(X) * sum(X)\n",
    "6. Gyroscope Y (stdev, max-min)\n",
    "7. Gyroscope Z (min, max, stdev, mean, max-min)\n",
    "8. Orientation Z (min, max, mean, max-min)\n",
    "\n",
    "*max-min was scaled for only positive values\n",
    "\n",
    "For my testing and iteration I utilized all 4 classifiers.  We only needed to use 2 of Decision Tree, Nearest Neighbor, and Support Vector Machines.  Utilizing all 4 helped to give me more information as I tested out various features and the relationships between the data.  My final selected feature set scores 100% accuracy with the required amount of classifiers laid out in the assignment.\n",
    "\n",
    "Now my goal was to get the best accuracy ratings with using the least amount of features.  As I started to narrow down my features, initially working with groups of 10 my prediction scores were perfect:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driving  Basic Scores\n",
      "log   =  1.0 ['Driving' 'Driving' 'Driving' 'Driving' 'Driving' 'Driving']\n",
      "tree  =  1.0 ['Driving' 'Driving' 'Driving' 'Driving' 'Driving' 'Driving']\n",
      "nbrs  =  1.0 ['Driving' 'Driving' 'Driving' 'Driving' 'Driving' 'Driving']\n",
      "svm   =  1.0 ['Driving' 'Driving' 'Driving' 'Driving' 'Driving' 'Driving']\n",
      "\n",
      "Running2  Basic Scores\n",
      "log   =  1.0 ['Running2' 'Running2' 'Running2' 'Running2' 'Running2' 'Running2']\n",
      "tree  =  1.0 ['Running2' 'Running2' 'Running2' 'Running2' 'Running2' 'Running2']\n",
      "nbrs  =  1.0 ['Running2' 'Running2' 'Running2' 'Running2' 'Running2' 'Running2']\n",
      "svm   =  1.0 ['Running2' 'Running2' 'Running2' 'Running2' 'Running2' 'Running2']\n",
      "\n",
      "Stairs  Basic Scores\n",
      "log   =  1.0 ['Stairs' 'Stairs' 'Stairs' 'Stairs' 'Stairs' 'Stairs']\n",
      "tree  =  1.0 ['Stairs' 'Stairs' 'Stairs' 'Stairs' 'Stairs' 'Stairs']\n",
      "nbrs  =  1.0 ['Stairs' 'Stairs' 'Stairs' 'Stairs' 'Stairs' 'Stairs']\n",
      "svm   =  1.0 ['Stairs' 'Stairs' 'Stairs' 'Stairs' 'Stairs' 'Stairs']\n",
      "\n",
      "Walking  Basic Scores\n",
      "log   =  1.0 ['Walking' 'Walking' 'Walking' 'Walking' 'Walking' 'Walking']\n",
      "tree  =  1.0 ['Walking' 'Walking' 'Walking' 'Walking' 'Walking' 'Walking']\n",
      "nbrs  =  1.0 ['Walking' 'Walking' 'Walking' 'Walking' 'Walking' 'Walking']\n",
      "svm   =  1.0 ['Walking' 'Walking' 'Walking' 'Walking' 'Walking' 'Walking']\n",
      "\n",
      "WebBrowsing  Basic Scores\n",
      "log   =  1.0 ['WebBrowsing' 'WebBrowsing' 'WebBrowsing' 'WebBrowsing' 'WebBrowsing'\n",
      " 'WebBrowsing']\n",
      "tree  =  1.0 ['WebBrowsing' 'WebBrowsing' 'WebBrowsing' 'WebBrowsing' 'WebBrowsing'\n",
      " 'WebBrowsing']\n",
      "nbrs  =  1.0 ['WebBrowsing' 'WebBrowsing' 'WebBrowsing' 'WebBrowsing' 'WebBrowsing'\n",
      " 'WebBrowsing']\n",
      "svm   =  1.0 ['WebBrowsing' 'WebBrowsing' 'WebBrowsing' 'WebBrowsing' 'WebBrowsing'\n",
      " 'WebBrowsing']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activities = ['Driving', 'Running2', 'Stairs', 'Walking', 'WebBrowsing']\n",
    "directory = '_DataFeaturesSmall/'\n",
    "\n",
    "def featuresXY(filename):\n",
    "    x = ''\n",
    "    y = [filename] * 24\n",
    "    xTest = ''\n",
    "    yTest = [filename] * 6\n",
    "    try:\n",
    "        data = np.genfromtxt((directory+filename+'.csv'), delimiter=',')\n",
    "        x = data[0:24,:]\n",
    "        xTest = data[24:30,:]\n",
    "    except Exception as e:\n",
    "        print('Exception featuresXY: ', str(e))\n",
    "    finally:\n",
    "        return x, y, xTest, yTest\n",
    "\n",
    "\n",
    "#Run through data...literally\n",
    "Xdriving, Ydriving, XdrivingTest, YdrivingTest = featuresXY(activities[0])\n",
    "Xrunning, Yrunning, XrunningTest, YrunningTest = featuresXY(activities[1]) #Running.csv I was wearing different pants\n",
    "Xstairs, Ystairs, XstairsTest, YstairsTest = featuresXY(activities[2])\n",
    "Xwalking, Ywalking, XwalkingTest, YwalkingTest = featuresXY(activities[3])\n",
    "Xweb, Yweb, XwebTest, YwebTest = featuresXY(activities[4])\n",
    "\n",
    "\n",
    "try:\n",
    "    XtrainMast = np.array(Xdriving)\n",
    "    XtrainMast = np.append(XtrainMast, Xrunning, 0)\n",
    "    XtrainMast = np.append(XtrainMast, Xstairs, 0)\n",
    "    XtrainMast = np.append(XtrainMast, Xwalking, 0)\n",
    "    XtrainMast = np.append(XtrainMast, Xweb, 0)\n",
    "\n",
    "    YtrainMast = np.array(Ydriving)\n",
    "    YtrainMast = np.append(YtrainMast, Yrunning, 0)\n",
    "    YtrainMast = np.append(YtrainMast, Ystairs, 0)\n",
    "    YtrainMast = np.append(YtrainMast, Ywalking, 0)\n",
    "    YtrainMast = np.append(YtrainMast, Yweb, 0)\n",
    "\n",
    "    XtestMast = np.array(XdrivingTest)\n",
    "    XtestMast = np.append(XtestMast, XrunningTest, 0)\n",
    "    XtestMast = np.append(XtestMast, XstairsTest, 0)\n",
    "    XtestMast = np.append(XtestMast, XwalkingTest, 0)\n",
    "    XtestMast = np.append(XtestMast, XwebTest, 0)\n",
    "\n",
    "    YtestMast = np.array(YdrivingTest)\n",
    "    YtestMast = np.append(YtestMast, YrunningTest, 0)\n",
    "    YtestMast = np.append(YtestMast, YstairsTest, 0)\n",
    "    YtestMast = np.append(YtestMast, YwalkingTest, 0)\n",
    "    YtestMast = np.append(YtestMast, YwebTest, 0)\n",
    "except Exception as e:\n",
    "    print('Exception append Arrays: ', str(e))\n",
    "    \n",
    "#Classifiers\n",
    "clfLog = LogisticRegression().fit(XtrainMast,YtrainMast)\n",
    "clfTree = tree.DecisionTreeClassifier().fit(XtrainMast,YtrainMast)\n",
    "clfNbrs = KNeighborsClassifier(n_neighbors=1).fit(XtrainMast,YtrainMast)\n",
    "clfSVM = svm.SVC(kernel='linear', C=2).fit(XtrainMast,YtrainMast)\n",
    "\n",
    "#run basic scores and prediction\n",
    "try:\n",
    "    for act in activities:\n",
    "        if act == activities[0]:\n",
    "            xTest,yTest = XdrivingTest,YdrivingTest\n",
    "        elif act == activities[1]:\n",
    "            xTest,yTest = XrunningTest,YrunningTest\n",
    "        elif act == activities[2]:\n",
    "            xTest,yTest =  XstairsTest,YstairsTest\n",
    "        elif act == activities[3]:\n",
    "            xTest,yTest = XwalkingTest,YwalkingTest\n",
    "        elif act == activities[4]:\n",
    "            xTest,yTest = XwebTest,YwebTest\n",
    "\n",
    "        print(act,' Basic Scores')\n",
    "        print('log   = ', clfLog.score(xTest, yTest), clfLog.predict(xTest))\n",
    "        print('tree  = ', clfTree.score(xTest, yTest), clfLog.predict(xTest))\n",
    "        print('nbrs  = ', clfNbrs.score(xTest, yTest), clfLog.predict(xTest))\n",
    "        print('svm   = ', clfSVM.score(xTest, yTest), clfLog.predict(xTest))\n",
    "        print('')\n",
    "except Exception as e:\n",
    "    print('Exception runBasicScores: ', str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These basic predictions are spot on. Once I got down to using less features I was predicting the activity with 100% accuracy using these basic means.  As I tried out different features 3 features consistently showed the most value:\n",
    "1. Acceleration X - Standard Deviation\n",
    "1. Acceleration X - Mean\n",
    "3. Average Resultant Acceleration (Acceleration X)\n",
    "\n",
    "Removing any of these three features decreased my scores.  I even tried Acceraltion Y and Z as well.  Those sensors often dropped my accuracy by 3-6 % points.  Since my phone was in my pocket facing the exact same direction for every activity it is not surpising one axis can so accuralety predict the data.\n",
    "\n",
    "* *Running the plot3D() code below can lock up the Python Kernel in Jupyter Notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot3D(title, Xlabel, Ylabel, Zlabel, inc, i):\n",
    "    #inc = 4 to go through XYZ for given sensor\n",
    "    #inc = 13 go through avg resultant acceleration\n",
    "    #i = starting point\n",
    "    ii = i + inc\n",
    "    iii = ii = inc\n",
    "    Xdr = np.genfromtxt((directory+activities[0]+'.csv'), delimiter=',')\n",
    "    Xru = np.genfromtxt((directory+activities[1]+'.csv'), delimiter=',')\n",
    "    Xst = np.genfromtxt((directory+activities[2]+'.csv'), delimiter=',')\n",
    "    Xwa = np.genfromtxt((directory+activities[3]+'.csv'), delimiter=',')\n",
    "    Xwe = np.genfromtxt((directory+activities[4]+'.csv'), delimiter=',')\n",
    "    try:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        #print(Xdr[:, 1])\n",
    "        xa = Xdr[:, i]\n",
    "        ya = Xdr[:, ii]\n",
    "        za = Xdr[:, iii]\n",
    "\n",
    "        xb = Xru[:, i]\n",
    "        yb = Xru[:, ii]\n",
    "        zb = Xru[:, iii]\n",
    "\n",
    "        xc = Xst[:, i]\n",
    "        yc = Xst[:, ii]\n",
    "        zc = Xst[:, iii]\n",
    "\n",
    "        xd = Xwa[:, i]\n",
    "        yd = Xwa[:, ii]\n",
    "        zd = Xwa[:, iii]\n",
    "\n",
    "        xe = Xwe[:, i]\n",
    "        ye = Xwe[:, ii]\n",
    "        ze = Xwe[:, iii]\n",
    "\n",
    "        dr = ax.scatter(xa, ya, za, c='red', marker='o', label='Driving')\n",
    "        ru = ax.scatter(xb, yb, zb, c='orange', marker='o', label='Running')\n",
    "        st = ax.scatter(xc, yc, zc, c='green', marker='o', label='Stairs')\n",
    "        wa = ax.scatter(xd, yd, zd, c='purple', marker='o', label='Walking')\n",
    "        we = ax.scatter(xe, ye, ze, c='blue', marker='^', label='WebBrowsing')\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('X '+ Xlabel)\n",
    "        ax.set_ylabel('Y '+ Ylabel)\n",
    "        ax.set_zlabel('Z '+ Zlabel)\n",
    "\n",
    "        plt.legend(handles=[dr, ru, st, wa, we], loc=3)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print('Exception plot3D: ', str(e))\n",
    "    finally:\n",
    "        return\n",
    "\n",
    "plot3D('Features 3D Plot', 'AccelX Stdev', 'AccelX Mean', 'Avg Resultant Accel (AccelX)', 1, 0) #inc, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Static Image of 3D Plot \n",
    "*(sometimes the 3D plot takes too long to execute in Jupyter Notebook)*\n",
    "![title](img/3d1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cross Validation + Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose for cross-validation is to ascertain any tuning parameters and to help avoid overfitting.  Due to our initial high number of experimental features it is necessary to go through these steps to see the usefullness of our features.  It will also allow us to train on all activities at once.  We can then test with another array containing 1 minute snippets from each activity. (*The testing data was removed from the training data) \n",
    "\n",
    "Next comes slightly tuning classifiers after reviewing the 3D plot.  As you can see above the data sits relatively in its own space.  However Driving and Web Browser; and Stairs and Walking are close to each other.  I was walking up a hill for my walking data.  Because of this it is necessary to increase the granularity of the Nearest Neighbors and SVM classifiers before we feed them into the Cross Validation and Confusion Matrix.\n",
    "\n",
    "Because I used a value of n_neighbors=1 for the cross validation.  Values of n_neighbors=5 worked perfectly fine for basic prediction.  However during my testing and data analysis to get 100% accuracy a more fine grained value for n_neighbors was needed.  The same logic applies to tweaking the C value for the Support Vector Machine classifier.  The larger the C valye the smaller the margin of the hyperplane used to slie and classify the data.  \n",
    "\n",
    "The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable.\n",
    "\n",
    "\n",
    "*The scores below use just Acceleration X - Standard Deviation, Acceleration X - Mean, and Average Resultant Acceleration (Acceleration X).*\n",
    "\n",
    "```\n",
    "Logistic Regression Confusion Matrix\n",
    "Scores:  [ 0.92  1.    1.    1.    1.  ]\n",
    "Accuracy Metrics: 0.9833\n",
    "CM Norm:  [[ 1.  0.  0.  0.  0.]\n",
    " [ 0.  1.  0.  0.  0.]\n",
    " [ 0.  0.  1.  0.  0.]\n",
    " [ 0.  0.  0.  1.  0.]\n",
    " [ 0.  0.  0.  0.  1.]]\n",
    "Accuracy CrossValScore: 0.9840 (+/- 0.0640)\n",
    "\n",
    "Decision Tree Confusion Matrix\n",
    "Scores:  [ 1.    1.    0.96  1.    1.  ]\n",
    "Accuracy Metrics: 0.9917\n",
    "CM Norm:  [[ 1.  0.  0.  0.  0.]\n",
    " [ 0.  1.  0.  0.  0.]\n",
    " [ 0.  0.  1.  0.  0.]\n",
    " [ 0.  0.  0.  1.  0.]\n",
    " [ 0.  0.  0.  0.  1.]]\n",
    "Accuracy CrossValScore: 0.9920 (+/- 0.0320)\n",
    "\n",
    "Nearest Neighbors Confusion Matrix\n",
    "Scores:  [ 0.96  1.    1.    1.    1.  ]\n",
    "Accuracy Metrics: 0.9917\n",
    "CM Norm:  [[ 1.  0.  0.  0.  0.]\n",
    " [ 0.  1.  0.  0.  0.]\n",
    " [ 0.  0.  1.  0.  0.]\n",
    " [ 0.  0.  0.  1.  0.]\n",
    " [ 0.  0.  0.  0.  1.]]\n",
    "Accuracy CrossValScore: 0.9920 (+/- 0.0320)\n",
    "\n",
    "Support Vector Machines Confusion Matrix\n",
    "Scores:  [ 1.  1.  1.  1.  1.]\n",
    "Accuracy Metrics: 1.0000\n",
    "CM Norm:  [[ 1.  0.  0.  0.  0.]\n",
    " [ 0.  1.  0.  0.  0.]\n",
    " [ 0.  0.  1.  0.  0.]\n",
    " [ 0.  0.  0.  1.  0.]\n",
    " [ 0.  0.  0.  0.  1.]]\n",
    "Accuracy CrossValScore: 1.0000 (+/- 0.0000)\n",
    "```\n",
    "\n",
    "My goal was to use as few features as possible.  While these three features are close they are not quite 100%.  I went through each of my preselected features adding 1 at time to see if they would positively effect the scores.  If this did not work I was goint to implement Some of my FFT code from breathing.py.  Almost all of the 4th features added had a negative effect on the accuracy scores.  However two improved the scores.  \n",
    "1. Gravity Y (mean)\n",
    "2. Gyroscope Y (stdev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gravity Y (mean) as 4th Feature\n",
    "```\n",
    "Logistic Regression Confusion Matrix\n",
    "Scores:  [ 0.92  1.    1.    1.    1.  ]\n",
    "Accuracy Metrics: 0.9833\n",
    "CM Norm:  [[ 1.  0.  0.  0.  0.]\n",
    " [ 0.  1.  0.  0.  0.]\n",
    " [ 0.  0.  1.  0.  0.]\n",
    " [ 0.  0.  0.  1.  0.]\n",
    " [ 0.  0.  0.  0.  1.]]\n",
    "Accuracy CrossValScore: 0.9840 (+/- 0.0640)\n",
    "\n",
    "Decision Tree Confusion Matrix\n",
    "Scores:  [ 1.  1.  1.  1.  1.]\n",
    "Accuracy Metrics: 0.9917\n",
    "CM Norm:  [[ 1.  0.  0.  0.  0.]\n",
    " [ 0.  1.  0.  0.  0.]\n",
    " [ 0.  0.  1.  0.  0.]\n",
    " [ 0.  0.  0.  1.  0.]\n",
    " [ 0.  0.  0.  0.  1.]]\n",
    "Accuracy CrossValScore: 1.0000 (+/- 0.0000)\n",
    "\n",
    "Nearest Neighbors Confusion Matrix\n",
    "Scores:  [ 1.  1.  1.  1.  1.]\n",
    "Accuracy Metrics: 1.0000\n",
    "CM Norm:  [[ 1.  0.  0.  0.  0.]\n",
    " [ 0.  1.  0.  0.  0.]\n",
    " [ 0.  0.  1.  0.  0.]\n",
    " [ 0.  0.  0.  1.  0.]\n",
    " [ 0.  0.  0.  0.  1.]]\n",
    "Accuracy CrossValScore: 1.0000 (+/- 0.0000)\n",
    "\n",
    "Support Vector Machines Confusion Matrix\n",
    "Scores:  [ 1.  1.  1.  1.  1.]\n",
    "Accuracy Metrics: 1.0000\n",
    "CM Norm:  [[ 1.  0.  0.  0.  0.]\n",
    " [ 0.  1.  0.  0.  0.]\n",
    " [ 0.  0.  1.  0.  0.]\n",
    " [ 0.  0.  0.  1.  0.]\n",
    " [ 0.  0.  0.  0.  1.]]\n",
    "Accuracy CrossValScore: 1.0000 (+/- 0.0000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gyroscope Y (stdev) as 4th Feature (BEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Confusion Matrix\n",
      "Scores:  [ 1.  1.  1.  1.  1.]\n",
      "Accuracy Metrics: 1.0000\n",
      "CM Norm:  [[ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Accuracy CrossValScore: 1.0000 (+/- 0.0000)\n",
      "\n",
      "Decision Tree Confusion Matrix\n",
      "Scores:  [ 1.    1.    0.96  1.    1.  ]\n",
      "Accuracy Metrics: 0.9917\n",
      "CM Norm:  [[ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Accuracy CrossValScore: 0.9920 (+/- 0.0320)\n",
      "\n",
      "Nearest Neighbors Confusion Matrix\n",
      "Scores:  [ 1.  1.  1.  1.  1.]\n",
      "Accuracy Metrics: 1.0000\n",
      "CM Norm:  [[ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Accuracy CrossValScore: 1.0000 (+/- 0.0000)\n",
      "\n",
      "Support Vector Machines Confusion Matrix\n",
      "Scores:  [ 1.  1.  1.  1.  1.]\n",
      "Accuracy Metrics: 1.0000\n",
      "CM Norm:  [[ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "Accuracy CrossValScore: 1.0000 (+/- 0.0000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def confusionMatrix(clfName, clf, Xtrain, Xtest, Ytrain, Ytest, color):\n",
    "    try:\n",
    "        clfRun = clf.fit(Xtrain, Ytrain).predict(Xtest)\n",
    "        scores = cross_validation.cross_val_score(clf, Xtrain, Ytrain, cv=5)\n",
    "\n",
    "        predicted = cross_validation.cross_val_predict(clf, Xtrain, Ytrain, cv=5)\n",
    "        acc = metrics.accuracy_score(Ytrain, predicted)\n",
    "\n",
    "        cm = confusion_matrix(Ytest, clfRun)\n",
    "        cmNorm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        plt.figure() #\n",
    "        plt.imshow(cmNorm, interpolation='nearest', cmap=color)\n",
    "        plt.title(clfName), plt.colorbar()\n",
    "        tick_marks = np.arange(len(activities))\n",
    "        plt.xticks(tick_marks, activities, rotation=45)\n",
    "        plt.yticks(tick_marks, activities)\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True'), plt.xlabel('Predicted')\n",
    "        print(clfName +  ' Confusion Matrix')\n",
    "        print('Scores: ',scores)\n",
    "        print('Accuracy Metrics: %0.4f' % (acc))\n",
    "        print('CM Norm: ',cmNorm)\n",
    "        print ('Accuracy CrossValScore: %0.4f (+/- %0.4f)' % (scores.mean(), scores.std() * 2))\n",
    "        print('')\n",
    "    except Exception as e:\n",
    "        print('Exception confusionMatrix: ', str(e))\n",
    "        \n",
    "#refresh clf for confusion matrix\n",
    "clfLog = LogisticRegression()\n",
    "clfTree = tree.DecisionTreeClassifier()\n",
    "clfNbrs = KNeighborsClassifier(n_neighbors=1)\n",
    "clfSVM = svm.SVC(kernel='linear', C=2)\n",
    "        \n",
    "confusionMatrix('Logistic Regression', clfLog, XtrainMast, XtestMast, YtrainMast, YtestMast, plt.cm.Blues)\n",
    "confusionMatrix('Decision Tree', clfTree, XtrainMast, XtestMast, YtrainMast, YtestMast, plt.cm.Reds)\n",
    "confusionMatrix('Nearest Neighbors', clfNbrs, XtrainMast, XtestMast, YtrainMast, YtestMast, plt.cm.Oranges)\n",
    "confusionMatrix('Support Vector Machines', clfSVM, XtrainMast, XtestMast, YtrainMast, YtestMast, plt.cm.Greens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix Graphics:\n",
    "Acceleration X - Standard Deviation, Acceleration X - Mean, \n",
    "Average Resultant Acceleration (Acceleration X), Gyroscope Y (stdev) \n",
    "\n",
    "![title](img/log.png)\n",
    "![title](img/tree.png)\n",
    "![title](img/nbrs.png)\n",
    "![title](img/svm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Analysis\n",
    "\n",
    "The assignment asked for us to use Logisitic Regression and two of the other classifiers.  I believe Gyroscope Y (stdev) was the best 4th feature to add because it fulfilled the project requirements.  It scored 100% on 3 required classifiers.  While Gravity Y (mean) is an interesting feature it did not score 100% on the requied Logistic Regression.  Its lowest accuracy of .9840 is lower than Gyroscope Y (stdev)'s lowest accuracy score of .9920.  Hence while it is close it is not the best.  \n",
    "\n",
    "Gyroscope Y makes sense as a good feature.  It captures the rotation and fine grained rotations of your leg.  It helps to capture these rotations for more exagerated movements such as Running, Stairs, and Walking.  It also helps distinguish Driving from Web Browsing.  Both are sendetary actions.  However driving at high speeds, such as my commute home work, generates distinct rotations that the Gyroscope is better equipped to capture than the accelerometer.  It was these fine grained differences the Accelerometer alone could not capture.  Hence why adding the Gyroscope helped boost the scores.\n",
    "\n",
    "I tried using Gravity Y (mean) and Gyroscope Y (stdev) as 4th and 5th features.  However the accuracy scores were identical ot just using Gravity Y (mean). Hence I stuck with only 4 features with Gyroscope Y (stdev) as the 4th one.  Less really is more in this case.  On a side note, Average Resultant Linear Acceleration and Average Resultant Gyroscope readings also proved to be effective as 4th features, but not as much as the other two.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pants Prediction + Analysis \n",
    "\n",
    "Pants are important:\n",
    "\n",
    "*\"Easy, guys.. I put my pants on just like the rest of you -- one leg at a time. Except, once my pants are on, I make gold records.\"* (Christopher Walken)\n",
    "\n",
    "\n",
    "Now my Running2.csv was used instead of Running.csv because in Running2 I wore my khaki work pants I wore while collecting the other data.  For Running I wore Gym Shorts.  Which caused my phone to have more exagerated movements as I ran.  I ran the same analysis I ran before, replacing Driving with Running.  It appears from the data my classifer can distinguish between similar activities where I was wearing different pants:\n",
    "\n",
    "```\n",
    "Running  Basic Scores\n",
    "log   =  1.0 ['Running' 'Running' 'Running' 'Running' 'Running' 'Running']\n",
    "tree  =  1.0 ['Running' 'Running' 'Running' 'Running' 'Running' 'Running']\n",
    "nbrs  =  1.0 ['Running' 'Running' 'Running' 'Running' 'Running' 'Running']\n",
    "svm   =  1.0 ['Running' 'Running' 'Running' 'Running' 'Running' 'Running']\n",
    "\n",
    "Running2  Basic Scores\n",
    "log   =  1.0 ['Running2' 'Running2' 'Running2' 'Running2' 'Running2' 'Running2']\n",
    "tree  =  1.0 ['Running2' 'Running2' 'Running2' 'Running2' 'Running2' 'Running2']\n",
    "nbrs  =  1.0 ['Running2' 'Running2' 'Running2' 'Running2' 'Running2' 'Running2']\n",
    "svm   =  1.0 ['Running2' 'Running2' 'Running2' 'Running2' 'Running2' 'Running2']\n",
    "\n",
    "SVM Cross Validation\n",
    "Scores:  [ 0.96  1.    1.    1.    1.  ]\n",
    "Accuracy:  0.9920 (+/- 0.0320)\n",
    "\n",
    "Decision Tree Cross Validation\n",
    "Scores:  [ 0.96  1.    1.    1.    0.95]\n",
    "Accuracy:  0.9820 (+/- 0.0445)\n",
    "\n",
    "Nearest Neighbors Cross Validation\n",
    "Scores:  [ 0.96  1.    1.    1.    1.  ]\n",
    "Accuracy:  0.9920 (+/- 0.0320)\n",
    "\n",
    "SVM Cross Validation\n",
    "Scores:  [ 0.96  1.    1.    1.    1.  ]\n",
    "Accuracy:  0.9920 (+/- 0.0320)\n",
    "```\n",
    "\n",
    "In cross validation wearing gym shorts did cause some error but the standard deviation for my gym shorts data was incredibly small.  Overally I would say the classifiers and features can distinguish between what pants I was weaing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Notes + Bugs\n",
    "\n",
    "**Running vs Runnng2 Data**\n",
    "\n",
    "In Running I was wearing gym shorts with my phone in my right pocket.  This caused the phone to move wildly.  In Running2 I was wearing the same pants I worke for all other acitivites.  My phone in my right pocket.  \n",
    "\n",
    "\n",
    "**Walking vs Walking2 Data**\n",
    "\n",
    "Walking was taken while walking to class.  It was partially up hill.  Walking2 was taken when I walked from my house to a coffee shop on April 11th.  I just recorded the data to have some extra data to look at it.\n",
    "\n",
    "\n",
    "**Possible Warnings**\n",
    "/Applications/anaconda/lib/python3.5/site-packages/matplotlib/tight_layout.py:222: UserWarning: tight_layout : falling back to Agg renderer\n",
    "  warnings.warn(\"tight_layout : falling back to Agg renderer\")\n",
    "  \n",
    "I found this was an occasional error on Mac with matplotlib. It is nothing serious.  So it is perfectly okay if it pops up in the terminal while looking through the code.  In this notebook this code was used to hide the warnings:\n",
    "```\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
